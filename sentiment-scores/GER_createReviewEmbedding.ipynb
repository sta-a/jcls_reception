{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62aaa3e7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Evaluative Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912275a3",
   "metadata": {},
   "source": [
    "### Implementation for German Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115007b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "import ast\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from random import sample\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "\n",
    "from itertools import cycle\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68060a",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pipe(doc):\n",
    "    tok_list = [tok.text.lower() for tok in doc\n",
    "                  if tok.is_alpha] \n",
    "    return tok_list\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=8):\n",
    "        preproc_pipe.append(tokenize_pipe(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')\n",
    "nlp.max_length = 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cce45",
   "metadata": {},
   "source": [
    "#### Reviews: Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d004d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_eng = r\"C:\\...\\GER_reviews\"\n",
    "additional_text = r\"C:\\...\\GER_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c492ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = glob.glob(reviews_eng + '/*.txt')\n",
    "list_additional = glob.glob(additional_text + '/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd11de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_txt = []\n",
    "for file_path in list_files:\n",
    "    with open(file_path, encoding=\"UTF-8\") as file:\n",
    "        reviews_txt.append(file.read())\n",
    "        \n",
    "additional_txt = []\n",
    "for file_path in list_additional:\n",
    "    with open(file_path, encoding=\"UTF-8\") as file:\n",
    "        additional_txt.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe51f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_txt_clean = [word.replace('\\n',' ') for word in reviews_txt]\n",
    "reviews_txt_clean = [word.lower() for word in reviews_txt_clean]\n",
    "\n",
    "#reviews = preprocess_pipe(reviews_txt_clean)\n",
    "\n",
    "#with open(r'C:\\...\\GER_reviews_tok.pkl', 'wb') as f:\n",
    "#    pickle.dump(reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce478738",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\GER_reviews_tok.pkl', 'rb') as f:\n",
    "    reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = [tok for sent in reviews for tok in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_reviews = pd.DataFrame(reviews_toks, columns=['Freq'])\n",
    "relfreq_reviews = pd.DataFrame(df_fdist_reviews['Freq'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128ff14",
   "metadata": {},
   "source": [
    "#### Comparative corpus: Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional_txt_clean = [word.replace('\\n',' ') for word in additional_txt]\n",
    "#additional_txt_clean = [word.lower() for word in additional_txt_clean]\n",
    "\n",
    "#additional = preprocess_pipe(additional_shuffled)\n",
    "\n",
    "#with open(r'C:...\\GER_additional_lemma.pkl', 'wb') as f:\n",
    "#    pickle.dump(additional_ger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\GER_additional_tok.pkl', 'rb') as f:\n",
    "    additional = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_toks = [tok for sent in additional for tok in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_additional = pd.DataFrame(additional_toks, columns=['Freq'])\n",
    "relfreq_additional = pd.DataFrame(df_fdist_additional['Freq'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06fbf4",
   "metadata": {},
   "source": [
    "#### Comparative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_novels = []\n",
    "in_reviews = []\n",
    "for row in relfreq_reviews.index:\n",
    "    freq_reviews = float(relfreq_reviews.loc[row])\n",
    "    freq_additional = 0\n",
    "    if row in relfreq_additional.index:\n",
    "        freq_additional = float(relfreq_additional.loc[row])\n",
    "    score = freq_reviews-freq_additional\n",
    "    #print(row + \": \" + str(freq_reviews) + \" | \" + str(freq_additional)) \n",
    "    if score > 0:\n",
    "        in_reviews.append([row, score])\n",
    "    else:\n",
    "        in_novels.append([row, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_reviews_words = []\n",
    "for entry in in_reviews:\n",
    "    in_reviews_words.append(entry[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af95278",
   "metadata": {},
   "source": [
    "#### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd59fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_toks = additional_toks + reviews_toks\n",
    "#sents = additional + reviews\n",
    "#model = Word2Vec(sentences = sents, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "#with open(r'C:\\...\\GER_reviews_model_FINAL.pkl', 'wb') as f:\n",
    "#    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f741ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\GER_reviews_model_FINAL.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5c536",
   "metadata": {},
   "source": [
    "#### Evaluative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\"ADJ\",\"NOUN\"]\n",
    "reviews_pos = []\n",
    "for doc in nlp.pipe(reviews_txt_clean, disable=[\"ner\"]):\n",
    "    sentence_lemmas = [[token.lemma_ for token in sent if token.pos_ in pos] for sent in doc.sents]\n",
    "    reviews_pos = reviews_pos + sentence_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8cc54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c83ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tok_clean = [[tok for tok in review if tok not in stopwords] for review in reviews_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14acf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = []\n",
    "for review in reviews_tok_clean:\n",
    "    for tok in review:\n",
    "        if tok in in_reviews_words:\n",
    "            reviews_toks.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec0d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_tok_clean_top = Counter(reviews_toks).most_common(10000)\n",
    "corpus_top = [i[0] for i in reviews_tok_clean_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\"anziehend\",\"genial\",\"geistreich\",\"angemessen\",\n",
    "       \"wahr\",\"poetisch\",\"gelungen\",\"ästhetisch\",\n",
    "       \"originell\",\"künstlerisch\",\"edel\",\n",
    "       \"großartig\",\"dichterisch\",\"meisterhaft\",\"wertvoll\",\n",
    "       \"tadellos\",\"wahrhaft\",\"ideal\",\"echt\",\"hervorragend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_pos = []\n",
    "for p in pos:\n",
    "    x = model.wv.most_similar(p, topn=10)\n",
    "    for entry in x:\n",
    "        add_pos.append(entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c69327",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = [\"misform\",\"überspannt\",\"dürftig\",\"seltsam\",\n",
    "      \"schädlich\",\"unfertig\",\"frech\",\"enttäuschung\",\n",
    "      \"schwäche\",\"tadel\",\"simpel\",\"übertrieben\",\"überflüssig\",\n",
    "      \"fehler\",\"niedrig\",\"grauenhaft\",\"umständlich\",\"oberflächlich\",\n",
    "      \"mittelmäßig\",\"unnatürlich\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_neg = []\n",
    "for p in neg:\n",
    "    x = model.wv.most_similar(p, topn=10)\n",
    "    for entry in x:\n",
    "        add_neg.append(entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47563f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words = pos + neg + add_pos + add_neg\n",
    "val_words = list(set(val_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cef13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remaining_words = []\n",
    "for word in val_words:\n",
    "    if word in corpus_top:\n",
    "        remaining_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bac333",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c70254",
   "metadata": {},
   "source": [
    "#### Review analysis / SentiArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for word in remaining_words:\n",
    "    l.append(model.wv[word])\n",
    "X = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f7f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "af = AffinityPropagation(random_state=1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b5209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words = [\"\"]*len(cluster_centers_indices)\n",
    "i = 0\n",
    "\n",
    "for label in labels:\n",
    "    cluster_num = int(label)\n",
    "    cluster_words[cluster_num] = cluster_words[cluster_num] + \" \" + remaining_words[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c0da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle(\"bgrcmykbgrcmykbgrcmykbgrcmyk\")\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + \".\")\n",
    "    plt.plot(\n",
    "        cluster_center[0],\n",
    "        cluster_center[1],\n",
    "        \"o\",\n",
    "        markerfacecolor=col,\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "        )\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654002bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check indeces and adapt if necessary\n",
    "pos_centroids = [X[cluster_centers_indices[14]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bde0d",
   "metadata": {},
   "source": [
    "' meisterstück system vorbild eigenthum kunstwerk',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b220cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check indeces and adapt if necessary\n",
    "neg_centroids = [X[cluster_centers_indices[15]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ef4a5",
   "metadata": {},
   "source": [
    "' billig allgemein interessant langweilig lächerlich',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db2444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity = []\n",
    "all_pos = []\n",
    "all_neg = []\n",
    "for word in corpus_top:\n",
    "    vec = model.wv[word]\n",
    "    \n",
    "    pos = 0\n",
    "    for centroid in pos_centroids:\n",
    "        pos = np.dot(centroid, vec)/(np.linalg.norm(centroid)* np.linalg.norm(vec))\n",
    "        all_pos.append(pos)\n",
    "    pos = pos/len(pos_centroids)\n",
    "    \n",
    "    neg = 0\n",
    "    for centroid in neg_centroids:\n",
    "        neg = np.dot(centroid, vec)/(np.linalg.norm(centroid)* np.linalg.norm(vec))\n",
    "        all_neg.append(neg)\n",
    "    neg = neg/len(neg_centroids)\n",
    "    \n",
    "    sim = pos-neg\n",
    "    similarity.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5851492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senti_dict = dict(zip(corpus_top,similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fa361",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = []\n",
    "for doc in nlp.pipe(reviews_txt_clean, disable=[\"ner\"]):\n",
    "    sentence_toks = [[token.lemma_ for token in sent if token.text not in stopwords and token.is_alpha] for sent in doc.sents]\n",
    "    reviews_toks.append(sentence_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sentiscores = []\n",
    "for review in reviews_toks:\n",
    "    sentiscores_review_level = []\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for sentence in review:\n",
    "        sentiscores_sent_level = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in senti_dict.keys():\n",
    "                score = senti_dict[word]\n",
    "                sentiscores_sent_level.append(score)\n",
    "                \n",
    "        if len(sentiscores_sent_level) != 0:\n",
    "            sentiscores_review_level.append(sum(sentiscores_sent_level)/len(sentiscores_sent_level))\n",
    "        else:\n",
    "            sentiscores_review_level.append(0)\n",
    "    \n",
    "    if len(sentiscores_review_level) != 0:\n",
    "        reviews_sentiscores.append(sum(sentiscores_review_level)/len(sentiscores_review_level))\n",
    "    else:\n",
    "        reviews_sentiscores.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e006f",
   "metadata": {},
   "source": [
    "#### Comparison: Sentiment analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02659bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sentiment_scores = []\n",
    "for review in reviews_toks:\n",
    "    sentiscores_review_level = []\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for sentence in review:\n",
    "        sentiscores_sent_level = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            score = TextBlob(word).sentiment.polarity\n",
    "            sentiscores_sent_level.append(score)\n",
    "                \n",
    "        if len(sentence) != 0:\n",
    "            sentiscores_review_level.append(sum(sentiscores_sent_level)/len(sentence))\n",
    "        else:\n",
    "            sentiscores_review_level.append(0)\n",
    "        \n",
    "    if len(review) != 0:\n",
    "        all_sentiment_scores.append(sum(sentiscores_review_level)/len(review))\n",
    "    else:\n",
    "        all_sentiment_scores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "journals = []\n",
    "text_ids = []\n",
    "for file in list_files:\n",
    "    filename = re.sub(r\".+reviews\\\\(.+).txt\", r\"\\1\", file)\n",
    "    filenames.append(filename)\n",
    "    journal = re.sub(r\"\\d{5}_\\d{4}_(.+?)_.+\", r\"\\1\", filename)\n",
    "    journals.append(journal)\n",
    "    text_id = re.sub(r\"(\\d{5})_.+\", r\"\\1\", filename)\n",
    "    text_ids.append(int(text_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_df = pd.DataFrame(\n",
    "    {'sentiscore_average': reviews_sentiscores,\n",
    "     'sentiment_Textblob': all_sentiment_scores,\n",
    "     'textfile': filenames,\n",
    "     'journal': journals,\n",
    "     'text_id': text_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75523f07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_senti_df.to_csv(r\"C:\\...\\GER_reviews_senti_FINAL.csv\",\n",
    "                        sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv(r'C:\\...\\GER_testset.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b595616",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.merge(testset, reviews_senti_df, how='inner', on=['textfile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7715933",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ec19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sentiscore = [[0], [0], \n",
    "                               [0], [0]]\n",
    "confusion_matrix_sentiscore = pd.DataFrame(confusion_matrix_sentiscore,\n",
    "                                          index = ['TP','FP','TN','FN'],\n",
    "                                          columns = ['count'])\n",
    "confusion_matrix_textblob = [[0], [0], \n",
    "                             [0], [0]]\n",
    "confusion_matrix_textblob = pd.DataFrame(confusion_matrix_textblob,\n",
    "                                          index = ['TP','FP','TN','FN'],\n",
    "                                          columns = ['count'])\n",
    "\n",
    "\n",
    "for index, row in eval_df.iterrows():\n",
    "    sentiscore = row['sentiscore_average']\n",
    "    textblob = row['sentiment_Textblob']\n",
    "    classified = row['class']\n",
    "    text_id = row['text_id']\n",
    "\n",
    "    if classified == 1:\n",
    "        if sentiscore > 0:\n",
    "            #print(str(text_id )+ \": TRUE POSITIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"TP\"] += 1\n",
    "        else:\n",
    "            #print(str(text_id )+ \": FALSE NEGATIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"FN\"] += 1\n",
    "            \n",
    "        if textblob > 0:\n",
    "            #print(str(text_id )+ \": TRUE POSITIVE: \" + str(classified) + \" : \" +  str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"TP\"] += 1\n",
    "        else:\n",
    "            #print(str(text_id )+ \": FALSE NEGATIVE: \" + str(classified) + \" : \" +  str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"FN\"] += 1\n",
    "            \n",
    "    else:\n",
    "        if sentiscore < 0:\n",
    "            #print(str(text_id )+ \": TRUE NEGATIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"TN\"] += 1\n",
    "        else:\n",
    "            #print(str(text_id )+ \": FALSE POSITIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"FP\"] += 1       \n",
    "\n",
    "        if textblob < 0:\n",
    "            #print(str(text_id )+ \": TRUE NEGATIVE: \" + str(classified) + \" : \" + str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"TN\"] += 1\n",
    "        else:\n",
    "            #print(str(text_id )+ \": FALSE POSITIVE: \" + str(classified) + \" : \" + str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"FP\"] += 1       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b704019",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sentiscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d93946",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_senti = int(confusion_matrix_sentiscore.loc[\"TP\"])\n",
    "tn_senti = int(confusion_matrix_sentiscore.loc[\"TN\"])\n",
    "fn_senti = int(confusion_matrix_sentiscore.loc[\"FN\"])\n",
    "fp_senti = int(confusion_matrix_sentiscore.loc[\"FP\"])\n",
    "\n",
    "recall_senti = tp_senti/(tp_senti + fn_senti)\n",
    "precision_senti = tp_senti/(tp_senti + fp_senti)\n",
    "accuracy_senti = (tp_senti + tn_senti) / 30\n",
    "\n",
    "print(\"Sentiscore:\\n\" + \n",
    "      \"Recall: \" + str(recall_senti) + \"\\n\" +\n",
    "      \"Precision: \" + str(precision_senti) + \"\\n\" +\n",
    "      \"Accuracy: \" + str(accuracy_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_textblob = int(confusion_matrix_textblob.loc[\"TP\"])\n",
    "tn_textblob = int(confusion_matrix_textblob.loc[\"TN\"])\n",
    "fn_textblob = int(confusion_matrix_textblob.loc[\"FN\"])\n",
    "fp_textblob = int(confusion_matrix_textblob.loc[\"FP\"])\n",
    "\n",
    "recall_textblob = tp_textblob/(tp_textblob + fn_textblob)\n",
    "precision_textblob = tp_textblob/(tp_textblob + fp_textblob)\n",
    "accuracy_textblob = (tp_textblob + tn_textblob) / 30\n",
    "\n",
    "print(\"TextBlob:\\n\" + \n",
    "      \"Recall: \" + str(recall_textblob) + \"\\n\" +\n",
    "      \"Precision: \" + str(precision_textblob) + \"\\n\" +\n",
    "      \"Accuracy: \" + str(accuracy_textblob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7eac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob = reviews_senti_df.loc[reviews_senti_df['sentiment_Textblob'] > 0]\n",
    "neg_senti = reviews_senti_df.loc[reviews_senti_df['sentiscore_average'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b875ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 1st quartile\n",
    "threshold_textBlob = pos_textBlob['sentiment_Textblob'].describe()[4]\n",
    "threshold_senti = neg_senti['sentiscore_average'].describe()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob = pos_textBlob.loc[pos_textBlob['sentiment_Textblob'] > threshold_textBlob/2]\n",
    "neg_senti = neg_senti.loc[neg_senti['sentiscore_average'] < threshold_senti/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob_reviews = list(pos_textBlob[\"textfile\"])\n",
    "neg_senti_reviews = list(neg_senti[\"textfile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bffafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob_exclusive = pos_textBlob.loc[~pos_textBlob['textfile'].isin(neg_senti_reviews)]\n",
    "pos_textBlob_exclusive[\"classified\"] = \"positive\"\n",
    "pos_textBlob_exclusive['sentiscore_average'] = 0\n",
    "\n",
    "neg_senti_exclusive = neg_senti.loc[~neg_senti['textfile'].isin(pos_textBlob_reviews)]\n",
    "neg_senti_exclusive[\"classified\"] = \"negative\"\n",
    "neg_senti_exclusive['sentiment_Textblob'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab54e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob_exclusive_reviews = list(pos_textBlob_exclusive[\"textfile\"])\n",
    "neg_senti_exclusive_reviews = list(neg_senti_exclusive[\"textfile\"])\n",
    "not_in = pos_textBlob_exclusive_reviews + neg_senti_exclusive_reviews\n",
    "\n",
    "not_classified = reviews_senti_df.loc[~reviews_senti_df['textfile'].isin(not_in)]\n",
    "not_classified[\"classified\"] = \"not_classified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pos_textBlob_exclusive,neg_senti_exclusive,not_classified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba620",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_textBlob_exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2aa99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(neg_senti_exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_classified = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e517c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_senti_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_classified.to_csv(r\"C:\\...\\GER_reviews_senti_classified.csv\",\n",
    "                                sep = ';', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
