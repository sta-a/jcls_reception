{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abca173b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Evaluative Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128ecd8",
   "metadata": {},
   "source": [
    "### Implementation for English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115007b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import glob\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from random import sample\n",
    "from itertools import chain\n",
    "import pickle\n",
    "\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from textblob import TextBlob\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pipe(doc):\n",
    "    tok_list = [tok.text.lower() for tok in doc\n",
    "                  if tok.is_alpha] \n",
    "    return tok_list\n",
    "\n",
    "def preprocess_pipe(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=8):\n",
    "        preproc_pipe.append(tokenize_pipe(doc))\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 1000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ed207",
   "metadata": {},
   "source": [
    "#### Reviews: Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d004d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_eng = r\"C:\\...\\ENG_reviews\"\n",
    "additional_text = r\"C:\\...\\ENG_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c492ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = glob.glob(reviews_eng + '/*.txt')\n",
    "list_additional = glob.glob(additional_text + '/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd11de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_txt = []\n",
    "for file_path in list_files:\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        reviews_txt.append(file.read())\n",
    "        \n",
    "additional_txt = []\n",
    "for file_path in list_additional:\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        additional_txt.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe51f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_txt_clean = [word.replace('\\n',' ') for word in reviews_txt]\n",
    "reviews_txt_clean = [word.lower() for word in reviews_txt_clean]\n",
    "\n",
    "#reviews = preprocess_pipe(reviews_txt_clean)\n",
    "\n",
    "#with open(r'C:\\...\\ENG_reviews_tok.pkl', 'wb') as f:\n",
    "#    pickle.dump(reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\ENG_reviews_tok.pkl', 'rb') as f:\n",
    "    reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = [tok for sent in reviews for tok in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_reviews = pd.DataFrame(reviews_toks, columns=['Freq'])\n",
    "relfreq_reviews = pd.DataFrame(df_fdist_reviews['Freq'].value_counts())\n",
    "relfreq_reviews = relfreq_reviews / len(reviews_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc5469",
   "metadata": {},
   "source": [
    "#### Comparative corpus: Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional_txt_clean = [word.replace('\\n',' ') for word in additional_txt]\n",
    "#additional_txt_clean = [word.lower() for word in additional_txt_clean]\n",
    "\n",
    "#additional = preprocess_pipe(additional_txt_clean)\n",
    "\n",
    "#with open(r'C:\\...\\ENG_additional_lemma.pkl', 'wb') as f:\n",
    "#    pickle.dump(additional, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\ENG_additional_tok.pkl', 'rb') as f:\n",
    "    additional = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8192c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_toks = [tok for sent in additional for tok in sent if tok.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_additional = pd.DataFrame(additional_toks, columns=['Freq'])\n",
    "relfreq_additional = pd.DataFrame(df_fdist_additional['Freq'].value_counts())\n",
    "relfreq_additional = relfreq_additional / len(additional_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75818827",
   "metadata": {},
   "source": [
    "#### Comparative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad3482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_novels = []\n",
    "in_reviews = []\n",
    "for row in relfreq_reviews.index:\n",
    "    freq_reviews = float(relfreq_reviews.loc[row])\n",
    "    freq_additional = 0\n",
    "    if row in relfreq_additional.index:\n",
    "        freq_additional = float(relfreq_additional.loc[row])\n",
    "    score = freq_reviews-freq_additional\n",
    "    if score > 0:\n",
    "        #print(row + \": \" + str(freq_reviews) + \" | \" + str(freq_additional)) \n",
    "        in_reviews.append([row, score])\n",
    "    else:\n",
    "        in_novels.append([row, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(in_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_reviews_words = []\n",
    "for entry in in_reviews:\n",
    "    in_reviews_words.append(entry[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34b7ed",
   "metadata": {},
   "source": [
    "#### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_toks = additional_toks + reviews_toks\n",
    "#sents = additional + reviews\n",
    "#model = Word2Vec(sentences = sents, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "#with open(r'C:\\...\\ENG_reviews_model_FINAL.pkl', 'wb') as f:\n",
    "#    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\...\\ENG_reviews_model_FINAL.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1d65c",
   "metadata": {},
   "source": [
    "#### Evaluative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\"ADJ\",\"NOUN\"]\n",
    "reviews_pos = []\n",
    "for doc in nlp.pipe(reviews_txt_clean, disable=[\"ner\"]):\n",
    "    sentence_toks = [[token.text for token in sent if token.pos_ in pos] for sent in doc.sents]\n",
    "    reviews_pos = reviews_pos + sentence_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b9dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_tok_clean = [[tok for tok in review if tok not in stopwords] for review in reviews_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b8cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = []\n",
    "for review in reviews_tok_clean:\n",
    "    for tok in review:\n",
    "        if tok in in_reviews_words:\n",
    "            reviews_toks.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec0d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_tok_clean_top = Counter(reviews_toks).most_common(10000)\n",
    "corpus_top = [i[0] for i in reviews_tok_clean_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [\"excellent\",\"admirable\",\"estimable\",\"exemplary\",\n",
    "       \"invaluable\",\"incomparable\",\"superb\",\"outstanding\",\n",
    "       \"wonderful\",\"perfect\",\"superior\",\"worthy\",\"fine\",\n",
    "       \"exceptional\",\"skillful\",\"masterful\",\"extraordinary\",\n",
    "       \"impressive\",\"notable\",\"noteworthy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad70d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_pos = []\n",
    "for p in pos:\n",
    "    x = model.wv.most_similar(p, topn=2)\n",
    "    for entry in x:\n",
    "        add_pos.append(entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c69327",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = [\"terrible\",\"grievous\",\"hideous\",\"ghastly\",\n",
    "       \"disgusting\",\"unfavourable\",\"disagreeable\",\"distasteful\",\n",
    "       \"error\",\"fault\",\"unpleasant\",\"imprudent\",\"unlikely\",\n",
    "       \"undesirable\",\"unreasonable\",\"absurd\",\"offensive\",\n",
    "       \"unsuitable\",\"questionable\",\"disconcerting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f82210",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_neg = []\n",
    "for p in neg:\n",
    "    x = model.wv.most_similar(p, topn=2)\n",
    "    for entry in x:\n",
    "        add_neg.append(entry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_words = pos + neg + add_pos + add_neg\n",
    "val_words = list(set(val_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cef13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remaining_words = []\n",
    "for word in val_words:\n",
    "    if word in corpus_top:\n",
    "        remaining_words.append(word)\n",
    "    #else:\n",
    "    #    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0214b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e9cb",
   "metadata": {},
   "source": [
    "#### Review analysis / SentiArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for word in remaining_words:\n",
    "    l.append(model.wv[word])\n",
    "X = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f7f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "af = AffinityPropagation(random_state=1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b5209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words = [\"\"]*len(cluster_centers_indices)\n",
    "i = 0\n",
    "\n",
    "for label in labels:\n",
    "    cluster_num = int(label)\n",
    "    cluster_words[cluster_num] = cluster_words[cluster_num] + \" \" + remaining_words[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd163e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle(\"bgrcmykbgrcmykbgrcmykbgrcmyk\")\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + \".\")\n",
    "    plt.plot(\n",
    "        cluster_center[0],\n",
    "        cluster_center[1],\n",
    "        \"o\",\n",
    "        markerfacecolor=col,\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "        )\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab358a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check indeces and adapt if necessary\n",
    "pos_centroids = [X[cluster_centers_indices[10]],X[cluster_centers_indices[2]],X[cluster_centers_indices[11]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31725ca",
   "metadata": {},
   "source": [
    "' extraordinary unusual remarkable singular',\n",
    "' startling marvellous wonderful astonishing'\n",
    "' admirable excellent amiable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b220cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check indeces and adapt if necessary\n",
    "neg_centroids = [X[cluster_centers_indices[3]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e1511e",
   "metadata": {},
   "source": [
    "' ghastly horrible hideous terrible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b02ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db2444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys = []\n",
    "similarity = []\n",
    "all_pos = []\n",
    "all_neg = []\n",
    "for word in corpus_top:\n",
    "    if word in vocab.keys():\n",
    "        vec = model.wv[word]\n",
    "\n",
    "        pos = 0\n",
    "        for centroid in pos_centroids:\n",
    "            pos = np.dot(centroid, vec)/(np.linalg.norm(centroid)* np.linalg.norm(vec))\n",
    "            all_pos.append(pos)\n",
    "        pos = pos/len(pos_centroids)\n",
    "\n",
    "        neg = 0\n",
    "        for centroid in neg_centroids:\n",
    "            neg = np.dot(centroid, vec)/(np.linalg.norm(centroid)* np.linalg.norm(vec))\n",
    "            all_neg.append(neg)\n",
    "        neg = neg/len(neg_centroids)\n",
    "\n",
    "        sim = pos-neg\n",
    "        similarity.append(sim)\n",
    "        keys.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5851492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senti_dict = dict(zip(keys,similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_toks = []\n",
    "for doc in nlp.pipe(reviews_txt_clean, disable=[\"ner\"]):\n",
    "    sentence_toks = [[token.text for token in sent if token.text not in stopwords and token.is_alpha] for sent in doc.sents]\n",
    "    reviews_toks.append(sentence_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_sentiscores = []\n",
    "for review in reviews_toks:\n",
    "    sentiscores_review_level = []\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for sentence in review:\n",
    "        sentiscores_sent_level = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word in senti_dict.keys():\n",
    "                score = senti_dict[word]\n",
    "                sentiscores_sent_level.append(score)\n",
    "                \n",
    "        if len(sentiscores_sent_level) != 0:\n",
    "            sentiscores_review_level.append(sum(sentiscores_sent_level)/len(sentiscores_sent_level))\n",
    "        else:\n",
    "            sentiscores_review_level.append(0)\n",
    "    \n",
    "    if len(sentiscores_review_level) != 0:\n",
    "        reviews_sentiscores.append(sum(sentiscores_review_level)/len(sentiscores_review_level))\n",
    "    else:\n",
    "        reviews_sentiscores.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45fb84",
   "metadata": {},
   "source": [
    "#### Comparison: Sentiment analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449a678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sentiment_scores = []\n",
    "for review in reviews_toks:\n",
    "    sentiscores_review_level = []\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for sentence in review:\n",
    "        sentiscores_sent_level = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            score = TextBlob(word).sentiment.polarity\n",
    "            if word in senti_dict.keys():\n",
    "                score = score * 1.5\n",
    "            sentiscores_sent_level.append(score)\n",
    "                \n",
    "        if len(sentence) != 0:\n",
    "            sentiscores_review_level.append(sum(sentiscores_sent_level)/len(sentence))\n",
    "        else:\n",
    "            sentiscores_review_level.append(0)\n",
    "        \n",
    "    if len(review) != 0:\n",
    "        all_sentiment_scores.append(sum(sentiscores_review_level)/len(review))\n",
    "    else:\n",
    "        all_sentiment_scores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "journals = []\n",
    "text_ids = []\n",
    "for file in list_files:\n",
    "    filename = re.sub(r\".+reviews\\\\(.+).txt\", r\"\\1\", file)\n",
    "    filenames.append(filename)\n",
    "    journal = re.sub(r\"\\d{5}_\\d{4}_(.+?)_.+\", r\"\\1\", filename)\n",
    "    journals.append(journal)\n",
    "    text_id = re.sub(r\"(\\d{5})_.+\", r\"\\1\", filename)\n",
    "    text_ids.append(int(text_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_df = pd.DataFrame(\n",
    "    {'sentiscore_average': reviews_sentiscores,\n",
    "     'sentiment_Textblob': all_sentiment_scores,\n",
    "     'textfile': filenames,\n",
    "     'journal': journals,\n",
    "     'text_id': text_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e79af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_df.to_csv(r\"C:\\...\\ENG_reviews_senti_FINAL.csv\",\n",
    "                        sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.read_csv(r'C:\\...\\ENG_testset.csv', sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2a84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_df = pd.merge(testset, reviews_senti_df, how='inner', on=['textfile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23aebfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bed0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix_sentiscore = [[0], [0], \n",
    "                               [0], [0]]\n",
    "confusion_matrix_sentiscore = pd.DataFrame(confusion_matrix_sentiscore,\n",
    "                                          index = ['TP','FP','TN','FN'],\n",
    "                                          columns = ['count'])\n",
    "confusion_matrix_textblob = [[0], [0], \n",
    "                             [0], [0]]\n",
    "confusion_matrix_textblob = pd.DataFrame(confusion_matrix_textblob,\n",
    "                                          index = ['TP','FP','TN','FN'],\n",
    "                                          columns = ['count'])\n",
    "\n",
    "\n",
    "for index, row in eval_df.iterrows():\n",
    "    sentiscore = row['sentiscore_average']\n",
    "    textblob = row['sentiment_Textblob']\n",
    "    classified = row['class']\n",
    "    text_id = row['text_id']\n",
    "\n",
    "    if classified == 1:\n",
    "        if sentiscore > 0:\n",
    "            print(str(text_id )+ \": TRUE POSITIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"TP\"] += 1\n",
    "        else:\n",
    "            print(str(text_id )+ \": FALSE NEGATIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"FN\"] += 1\n",
    "            \n",
    "        if textblob > 0:\n",
    "            print(str(text_id )+ \": TRUE POSITIVE: \" + str(classified) + \" : \" +  str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"TP\"] += 1\n",
    "        else:\n",
    "            print(str(text_id )+ \": FALSE NEGATIVE: \" + str(classified) + \" : \" +  str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"FN\"] += 1\n",
    "            \n",
    "    else:\n",
    "        if sentiscore < 0:\n",
    "            print(str(text_id )+ \": TRUE NEGATIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"TN\"] += 1\n",
    "        else:\n",
    "            print(str(text_id )+ \": FALSE POSITIVE: \" + str(classified) + \" : \" + str(sentiscore))\n",
    "            confusion_matrix_sentiscore.loc[\"FP\"] += 1       \n",
    "\n",
    "        if textblob < 0:\n",
    "            print(str(text_id )+ \": TRUE NEGATIVE: \" + str(classified) + \" : \" + str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"TN\"] += 1\n",
    "        else:\n",
    "            print(str(text_id )+ \": FALSE POSITIVE: \" + str(classified) + \" : \" + str(textblob))\n",
    "            confusion_matrix_textblob.loc[\"FP\"] += 1       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608be257",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_sentiscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a187ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_senti = int(confusion_matrix_sentiscore.loc[\"TP\"])\n",
    "tn_senti = int(confusion_matrix_sentiscore.loc[\"TN\"])\n",
    "fn_senti = int(confusion_matrix_sentiscore.loc[\"FN\"])\n",
    "fp_senti = int(confusion_matrix_sentiscore.loc[\"FP\"])\n",
    "\n",
    "recall_senti = tp_senti/(tp_senti + fn_senti)\n",
    "precision_senti = tp_senti/(tp_senti + fp_senti)\n",
    "accuracy_senti = (tp_senti + tn_senti) / 30\n",
    "\n",
    "print(\"Sentiscore:\\n\" + \n",
    "      \"Recall: \" + str(recall_senti) + \"\\n\" +\n",
    "      \"Precision: \" + str(precision_senti) + \"\\n\" +\n",
    "      \"Accuracy: \" + str(accuracy_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_textblob = int(confusion_matrix_textblob.loc[\"TP\"])\n",
    "tn_textblob = int(confusion_matrix_textblob.loc[\"TN\"])\n",
    "fn_textblob = int(confusion_matrix_textblob.loc[\"FN\"])\n",
    "fp_textblob = int(confusion_matrix_textblob.loc[\"FP\"])\n",
    "\n",
    "recall_textblob = tp_textblob/(tp_textblob + fn_textblob)\n",
    "precision_textblob = tp_textblob/(tp_textblob + fp_textblob)\n",
    "accuracy_textblob = (tp_textblob + tn_textblob) / 30\n",
    "\n",
    "print(\"TextBlob:\\n\" + \n",
    "      \"Recall: \" + str(recall_textblob) + \"\\n\" +\n",
    "      \"Precision: \" + str(precision_textblob) + \"\\n\" +\n",
    "      \"Accuracy: \" + str(accuracy_textblob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae0fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_senti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ff9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob = reviews_senti_df.loc[reviews_senti_df['sentiment_Textblob'] > 0]\n",
    "neg_senti = reviews_senti_df.loc[reviews_senti_df['sentiscore_average'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d91bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 1st quartile\n",
    "threshold_textBlob = pos_textBlob['sentiment_Textblob'].describe()[4]\n",
    "threshold_senti = neg_senti['sentiscore_average'].describe()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob = pos_textBlob.loc[pos_textBlob['sentiment_Textblob'] > threshold_textBlob/2]\n",
    "neg_senti = neg_senti.loc[neg_senti['sentiscore_average'] < threshold_senti/2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_textBlob_reviews = list(pos_textBlob[\"textfile\"])\n",
    "neg_senti_reviews = list(neg_senti[\"textfile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd646a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_textBlob_exclusive = pos_textBlob.loc[~pos_textBlob['textfile'].isin(neg_senti_reviews)]\n",
    "pos_textBlob_exclusive[\"classified\"] = \"positive\"\n",
    "pos_textBlob_exclusive['sentiscore_average'] = 0\n",
    "\n",
    "neg_senti_exclusive = neg_senti.loc[~neg_senti['textfile'].isin(pos_textBlob_reviews)]\n",
    "neg_senti_exclusive[\"classified\"] = \"negative\"\n",
    "neg_senti_exclusive['sentiment_Textblob'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d08283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_textBlob_exclusive_reviews = list(pos_textBlob_exclusive[\"textfile\"])\n",
    "neg_senti_exclusive_reviews = list(neg_senti_exclusive[\"textfile\"])\n",
    "not_in = pos_textBlob_exclusive_reviews + neg_senti_exclusive_reviews\n",
    "\n",
    "not_classified = reviews_senti_df.loc[~reviews_senti_df['textfile'].isin(not_in)]\n",
    "not_classified[\"classified\"] = \"not_classified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pos_textBlob_exclusive,neg_senti_exclusive,not_classified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_textBlob_exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6afd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_senti_exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_classified = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59086bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_senti_classified.to_csv(r\"C:\\...\\ENG_reviews_senti_classified.csv\",\n",
    "                                sep = ';', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
