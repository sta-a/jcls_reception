{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48086b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "lang = 'eng'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from feature_extraction.doc2vec_chunk_vectorizer import Doc2VecChunkVectorizer\n",
    "from feature_extraction.doc_based_feature_extractor import DocBasedFeatureExtractor\n",
    "from feature_extraction.corpus_based_feature_extractor import CorpusBasedFeatureExtractor\n",
    "from utils import get_doc_paths\n",
    "\n",
    "raw_docs_dir = f'../data/raw_docs/{lang}/'\n",
    "features_dir = f'../data/features/{lang}/'\n",
    "\n",
    "if not os.path.exists(features_dir):\n",
    "    os.makedirs(features_dir)\n",
    "\n",
    "doc_paths = get_doc_paths(raw_docs_dir, lang)\n",
    "\n",
    "sentences_per_chunk = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc2vec embeddings\n",
    "d2vcv =  (lang, sentences_per_chunk)\n",
    "d2vcv.fit_transform(doc_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508d994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Document-based features\n",
    "document_chunk_features = []\n",
    "document_book_features = [] \n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk)\n",
    "    chunk_features, book_features = fe.get_all_features()  \n",
    "    document_chunk_features.extend(chunk_features)\n",
    "    document_book_features.append(book_features)\n",
    "print(len(document_book_features), len(document_chunk_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db6734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "document_chunk_features_fulltext = []\n",
    "\n",
    "for doc_path in tqdm(doc_paths):\n",
    "    fe = DocBasedFeatureExtractor(lang, doc_path, sentences_per_chunk=None)\n",
    "    chunk_features_fulltext, _ = fe.get_all_features()\n",
    "    document_chunk_features_fulltext.extend(chunk_features_fulltext)\n",
    "print(len(document_chunk_features_fulltext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle document-based features\n",
    "with open(features_dir + 'document_chunk_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(document_chunk_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'document_book_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(document_book_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'document_chunk_features_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(document_chunk_features_fulltext, f, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load document-based features  \n",
    "# with open(features_dir + 'document_chunk_features' + '.pkl', 'rb') as f:\n",
    "#     document_chunk_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'document_book_features' + '.pkl', 'rb') as f:\n",
    "#     document_book_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'document_chunk_features_fulltext' + '.pkl', 'rb') as f:\n",
    "#     document_chunk_features_fulltext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73136a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Corpus-based features\n",
    "cbfe = CorpusBasedFeatureExtractor(lang, doc_paths, sentences_per_chunk, nr_features=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3d69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_chunk_features, corpus_book_features = cbfe.get_all_features()\n",
    "\n",
    "with open(features_dir + 'corpus_chunk_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_chunk_features, f, -1)\n",
    "\n",
    "with open(features_dir + 'corpus_book_features' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_book_features, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the chunk features for the whole book, which is considered as one chunk\n",
    "cbfe_fulltext = CorpusBasedFeatureExtractor(lang, doc_paths, sentences_per_chunk=None, nr_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunk_features_fulltext, _ = cbfe_fulltext.get_all_features()\n",
    "with open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_chunk_features_fulltext, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load corpus-based features  \n",
    "# with open(features_dir + 'corpus_chunk_features' + '.pkl', 'rb') as f:\n",
    "#     corpus_chunk_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'corpus_book_features' + '.pkl', 'rb') as f:\n",
    "#     corpus_book_features = pickle.load(f)\n",
    "\n",
    "# with open(features_dir + 'corpus_chunk_features_fulltext' + '.pkl', 'rb') as f:\n",
    "#     corpus_chunk_features_fulltext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713327fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book features\n",
    "document_book_features = pd.DataFrame(document_book_features)\n",
    "document_chunk_features_fulltext = pd.DataFrame(document_chunk_features_fulltext)\n",
    "book_df = document_book_features\\\n",
    "            .merge(right=document_chunk_features_fulltext, on='file_name', how='outer', validate='one_to_one')\\\n",
    "            .merge(right=corpus_book_features, on='file_name', validate='one_to_one')\\\n",
    "            .merge(right=corpus_chunk_features_fulltext, on='file_name', validate='one_to_one')\n",
    "\n",
    "# Chunk features\n",
    "document_chunk_features = pd.DataFrame(document_chunk_features)\n",
    "chunk_df = document_chunk_features.merge(right=corpus_chunk_features, on='file_name', how='outer', validate='one_to_one')\n",
    "\n",
    "# Remove chunk id from file_name\n",
    "chunk_df['file_name'] = chunk_df['file_name'].str.split('_').str[:4].str.join('_')\n",
    "\n",
    "# Combine book features and averages of chunksaveraged chunk features\n",
    "book_and_averaged_chunk_df = book_df.merge(chunk_df.groupby('file_name').mean().reset_index(drop=False), on='file_name')\n",
    "\n",
    "chunk_and_copied_book_df = chunk_df.merge(right=book_df, on='file_name', how='outer', validate='many_to_one')\n",
    "\n",
    "dfs = {'book_df': book_df, 'book_and_averaged_chunk_df': book_and_averaged_chunk_df, 'chunk_df': chunk_df, 'chunk_and_copied_book_df': chunk_and_copied_book_df}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df = df.sort_values(by='file_name', axis=0, ascending=True, na_position='first')\n",
    "    df.to_csv(f'{features_dir}{name}.csv', index=False)\n",
    "    \n",
    "    #print(df.isnull().values.any())\n",
    "    #print(df.columns[df.isna().any()].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b3038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdd6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fbfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dde37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0d256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce0ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
